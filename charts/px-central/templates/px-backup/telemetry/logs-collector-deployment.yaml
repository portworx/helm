{{/* PX-Backup Telemetry Logs Collector Deployment */}}
{{- $pxBackupEnabled := .Values.pxbackup.enabled | default false }}
{{- $telemetryEnabled := .Values.pxbackup.telemetry.enabled | default false }}
{{- $logsEnabled := .Values.pxbackup.telemetry.logsCollector.enabled | default true }}
{{- if and (eq $pxBackupEnabled true) (eq $telemetryEnabled true) (eq $logsEnabled true) }}
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: px-backup-telemetry-logs-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: px-backup-telemetry
    app.kubernetes.io/name: logs-collector
{{- include "px-central.labels" . | nindent 4 }}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: px-backup-telemetry-logs-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: px-backup-telemetry
    app.kubernetes.io/name: logs-collector
{{- include "px-central.labels" . | nindent 4 }}
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["px-backup-telemetry-config", "px-backup-telemetry-logs-envoy-config"]
  verbs: ["get", "patch", "update"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["secrets"]
  resourceNames: ["pure-telemetry-certs"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["pods/exec"]
  verbs: ["create"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: px-backup-telemetry-logs-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: px-backup-telemetry
    app.kubernetes.io/name: logs-collector
{{- include "px-central.labels" . | nindent 4 }}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: px-backup-telemetry-logs-collector
subjects:
- kind: ServiceAccount
  name: px-backup-telemetry-logs-collector
  namespace: {{ .Release.Namespace }}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: px-backup-telemetry-logs-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: px-backup-telemetry
    app.kubernetes.io/name: logs-collector
{{- include "px-central.labels" . | nindent 4 }}
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/component: px-backup-telemetry
      app.kubernetes.io/name: logs-collector
  template:
    metadata:
      labels:
        app.kubernetes.io/component: px-backup-telemetry
        app.kubernetes.io/name: logs-collector
        app.kubernetes.io/instance: {{ .Release.Name | quote }}
        app.kubernetes.io/managed-by: {{ .Release.Service | quote }}
        helm.sh/chart: {{ include "px-central.chart" . }}
        app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
    spec:
      serviceAccountName: px-backup-telemetry-logs-collector
      initContainers:
      # Init container 1: Update Envoy ConfigMap with appliance-id (fixes Issue #2)
      - name: update-envoy-config
        image: bitnami/kubectl:latest
        imagePullPolicy: IfNotPresent
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "Waiting for appliance-id to be available..."

          # Wait for appliance-id to be populated (max 5 minutes)
          for i in {1..60}; do
            APPLIANCE_ID=$(kubectl get configmap -n {{ .Release.Namespace }} px-backup-telemetry-config -o jsonpath='{.data.appliance-id}' 2>/dev/null || echo "")
            if [ -n "$APPLIANCE_ID" ] && [ "$APPLIANCE_ID" != "null" ] && [ "$APPLIANCE_ID" != "unknown" ]; then
              echo "Found appliance-id: $APPLIANCE_ID"
              break
            fi
            echo "Waiting for appliance-id... (attempt $i/60)"
            sleep 5
          done

          if [ -z "$APPLIANCE_ID" ] || [ "$APPLIANCE_ID" == "null" ] || [ "$APPLIANCE_ID" == "unknown" ]; then
            echo "ERROR: appliance-id not found after 5 minutes"
            exit 1
          fi

          # Update Envoy ConfigMap with the correct appliance-id
          echo "Updating Envoy ConfigMap with appliance-id: $APPLIANCE_ID"
          kubectl get configmap -n {{ .Release.Namespace }} px-backup-telemetry-logs-envoy-config -o yaml | \
            sed "s/value: \"unknown\"/value: \"$APPLIANCE_ID\"/" | \
            kubectl apply -f -

          echo "Envoy ConfigMap updated successfully"
        securityContext:
          runAsUser: 1111

      # Init container 2: Wait for PX-Backup pod, then wait for appliance-id (fixes Issue #1)
      - name: wait-for-appliance-id
        image: bitnami/kubectl:latest
        imagePullPolicy: IfNotPresent
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "=== Step 1: Waiting for PX-Backup pod to be ready ==="

          # Wait for PX-Backup pod to be ready (max 5 minutes)
          for i in {1..60}; do
            PX_BACKUP_READY=$(kubectl get pods -n {{ .Release.Namespace }} -l app=px-backup -o jsonpath='{.items[0].status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "")

            if [ "$PX_BACKUP_READY" = "True" ]; then
              echo "✓ PX-Backup pod is ready"
              break
            fi

            echo "  Waiting for PX-Backup pod to be ready... (attempt $i/60)"
            sleep 5
          done

          if [ "$PX_BACKUP_READY" != "True" ]; then
            echo "✗ ERROR: PX-Backup pod is not ready after 5 minutes"
            echo "✗ Please check PX-Backup pod status"
            exit 1
          fi

          echo ""
          echo "=== Step 2: Waiting for appliance-id to be populated ==="

          # Wait for appliance-id to be populated (max 2 minutes after PX-Backup is ready)
          for i in {1..24}; do
            APPLIANCE_ID=$(kubectl get configmap -n {{ .Release.Namespace }} px-backup-telemetry-config -o jsonpath='{.data.appliance-id}' 2>/dev/null || echo "")

            if [ -n "$APPLIANCE_ID" ] && [ "$APPLIANCE_ID" != "null" ] && [ "$APPLIANCE_ID" != "unknown" ]; then
              echo "✓ Appliance-id is ready: $APPLIANCE_ID"
              echo "✓ Proceeding to certificate wait..."
              exit 0
            fi

            echo "  Waiting for appliance-id... (attempt $i/24)"
            sleep 5
          done

          echo "✗ ERROR: Timeout waiting for appliance-id after 2 minutes"
          echo "✗ PX-Backup pod is ready but appliance-id was not populated"
          echo "✗ Please check PX-Backup logs for telemetry initialization errors"
          exit 1
        securityContext:
          runAsUser: 1111

      # Init container 3: Wait for Pure1 certificate to be issued (NEW - prevents premature uploads)
      - name: wait-for-certificate
        image: bitnami/kubectl:latest
        imagePullPolicy: IfNotPresent
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo ""
          echo "=== Step 3: Waiting for Pure1 certificate to be issued ==="

          # Wait for Pure1 certificate to be issued (max 3 minutes after appliance-id)
          for i in {1..36}; do
            # Check if the secret exists and has a certificate
            CERT=$(kubectl get secret -n {{ .Release.Namespace }} pure-telemetry-certs -o jsonpath='{.data.cert}' 2>/dev/null || echo "")

            if [ -n "$CERT" ]; then
              echo "✓ Pure1 certificate is available"

              # Verify certificate is valid (not empty after base64 decode)
              CERT_DECODED=$(echo "$CERT" | base64 -d 2>/dev/null || echo "")
              if [ -n "$CERT_DECODED" ]; then
                echo "✓ Certificate is valid and ready for use"
                echo "✓ Log collector can now upload logs to Pure1"
                echo ""
                echo "✅ All initialization steps complete!"
                exit 0
              fi
            fi

            echo "  Waiting for Pure1 certificate... (attempt $i/36)"
            sleep 5
          done

          echo "✗ ERROR: Timeout waiting for Pure1 certificate after 3 minutes"
          echo "✗ Appliance-id is available but certificate was not issued"
          echo "✗ Please check registration pod logs for Pure1 registration errors"
          exit 1
        securityContext:
          runAsUser: 1111
      containers:
      # Container 1: Log Collector
      - name: log-collector
        image: {{ .Values.pxbackup.telemetry.logsCollector.image.registry }}/{{ .Values.pxbackup.telemetry.logsCollector.image.imageName }}:{{ .Values.pxbackup.telemetry.logsCollector.image.tag }}
        imagePullPolicy: {{ .Values.pxbackup.telemetry.logsCollector.image.imagePullPolicy | default "IfNotPresent" }}
        command: ["/bin/bash", "/scripts/collect-logs.sh"]
        securityContext:
          runAsUser: 1111
        volumeMounts:
        - name: log-collector-script
          mountPath: /scripts
          readOnly: true
        - name: log-storage
          mountPath: /var/cores
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      
      # Container 2: Log Upload Service
      - name: log-upload-service
        image: {{ .Values.pxbackup.telemetry.logsCollector.logUploadImage.registry | default "docker.io" }}/{{ .Values.pxbackup.telemetry.logsCollector.logUploadImage.imageName }}:{{ .Values.pxbackup.telemetry.logsCollector.logUploadImage.tag }}
        imagePullPolicy: {{ .Values.pxbackup.telemetry.logsCollector.logUploadImage.imagePullPolicy | default "IfNotPresent" }}
        ports:
        - name: grpc
          containerPort: 9090
          protocol: TCP
        env:
        - name: CONFIG_FILE
          value: "/config/ccm.properties"
        - name: APPLIANCE_ID
          valueFrom:
            configMapKeyRef:
              name: px-backup-telemetry-config
              key: appliance-id
        - name: APPLIANCE_NAME
          value: "px-backup-{{ .Release.Namespace }}"
        securityContext:
          runAsUser: 1111
        volumeMounts:
        - name: log-storage
          mountPath: /var/cores
        - name: log-upload-config
          mountPath: /config
          readOnly: true
        - name: log-upload-cache
          mountPath: /var/cache/ccm
        - name: phonehome-cache
          mountPath: /var/cache
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      
      # Container 3: Envoy Proxy
      - name: envoy
        image: {{ .Values.pxbackup.telemetry.logsCollector.envoyImage.registry }}/{{ .Values.pxbackup.telemetry.logsCollector.envoyImage.imageName }}:{{ .Values.pxbackup.telemetry.logsCollector.envoyImage.tag }}
        imagePullPolicy: {{ .Values.pxbackup.telemetry.logsCollector.envoyImage.imagePullPolicy | default "IfNotPresent" }}
        args:
        - envoy
        - --base-id
        - "3"
        - --config-path
        - /etc/envoy/envoy-config.yaml
        ports:
        - name: logs-proxy
          containerPort: 12002
          protocol: TCP
        - name: admin
          containerPort: 9901
          protocol: TCP
        securityContext:
          runAsUser: 1111
        volumeMounts:
        - name: envoy-config
          mountPath: /etc/envoy/envoy-config.yaml
          subPath: envoy-config.yaml
          readOnly: true
        - name: envoy-sds-config
          mountPath: /etc/envoy/tls_certificate_sds_secret.yaml
          subPath: tls_certificate_sds_secret.yaml
          readOnly: true
        - name: telemetry-certs
          mountPath: /appliance-cert
          readOnly: true
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      
      volumes:
      - name: log-collector-script
        configMap:
          name: px-backup-telemetry-log-collector-script
          defaultMode: 0755
      - name: log-storage
        emptyDir: {}  # Shared between log-collector and log-upload-service
      - name: log-upload-config
        configMap:
          name: px-backup-telemetry-logs-upload-config
      - name: log-upload-cache
        emptyDir: {}  # For SQLite bookkeeping
      - name: phonehome-cache
        emptyDir: {}  # For phonehome.sent tracking file (fixes Issue #3)
      - name: envoy-config
        configMap:
          name: px-backup-telemetry-logs-envoy-config
      - name: envoy-sds-config
        configMap:
          name: px-backup-telemetry-logs-envoy-sds-config
      - name: telemetry-certs
        secret:
          secretName: pure-telemetry-certs
          optional: true  # Secret is created by registration pod at runtime
{{- end }}


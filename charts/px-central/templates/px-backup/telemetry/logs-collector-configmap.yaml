{{/* PX-Backup Telemetry Logs Collector ConfigMaps */}}
{{- $pxBackupEnabled := .Values.pxbackup.enabled | default false }}
{{- $telemetryEnabled := .Values.pxbackup.telemetry.enabled | default false }}
{{- $logsEnabled := .Values.pxbackup.telemetry.logsCollector.enabled | default true }}
{{- if and (eq $pxBackupEnabled true) (eq $telemetryEnabled true) (eq $logsEnabled true) }}
---
# Log Collection Script
apiVersion: v1
kind: ConfigMap
metadata:
  name: px-backup-telemetry-log-collector-script
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: px-backup-telemetry
    app.kubernetes.io/name: logs-collector
{{- include "px-central.labels" . | nindent 4 }}
data:
  collect-logs.sh: |
    #!/bin/bash
    set -e

    LOG_DIR="/var/cores"
    NAMESPACE="{{ .Release.Namespace }}"
    POD_LABEL="app.kubernetes.io/part-of=px-backup"
    INTERVAL={{ .Values.pxbackup.telemetry.logsCollector.collectionInterval | default 3600 }}

    # Exclude patterns (regex) - pods matching these patterns will be excluded from log collection
    # Excludes: PostgreSQL, MySQL, frontend, install hooks, maintenance jobs, prometheus, alertmanager, telemetry pods
    EXCLUDE_PATTERNS="postgresql|mysql|frontend|post-install|maintenance-repo|prometheus|alertmanager|telemetry"

    echo "PX-Backup Log Collector started"
    echo "Collection interval: ${INTERVAL} seconds"
    echo "Pod label selector: ${POD_LABEL}"
    echo "Exclude patterns: ${EXCLUDE_PATTERNS}"

    # Install grpcurl if not present
    if ! command -v grpcurl &> /dev/null; then
        echo "Installing grpcurl..."
        mkdir -p /tmp/grpcurl
        curl -sL https://github.com/fullstorydev/grpcurl/releases/download/v1.8.9/grpcurl_1.8.9_linux_x86_64.tar.gz | tar xz -C /tmp/grpcurl
        chmod +x /tmp/grpcurl/grpcurl
        export PATH="/tmp/grpcurl:$PATH"
    fi

    collect_logs() {
        # Use 10-digit timestamp (YYYYMMDDHH) - hour-based format required by log-upload-service
        TIMESTAMP=$(date +%Y%m%d%H)

        echo "$(date): Collecting logs with timestamp $TIMESTAMP"

        # Get all pods matching the label selector
        ALL_PODS=$(kubectl get pods -n "$NAMESPACE" -l "$POD_LABEL" -o jsonpath='{.items[*].metadata.name}')

        if [ -z "$ALL_PODS" ]; then
            echo "No pods found matching label: $POD_LABEL"
            return
        fi

        echo "Found pods: $ALL_PODS"

        # Filter out excluded pods based on exclude patterns
        FILTERED_PODS=""
        EXCLUDED_COUNT=0
        INCLUDED_COUNT=0

        for POD in $ALL_PODS; do
            # Check if pod matches any exclude pattern
            if echo "$POD" | grep -qE "$EXCLUDE_PATTERNS"; then
                echo "  Excluding pod: $POD (matches exclude pattern)"
                EXCLUDED_COUNT=$((EXCLUDED_COUNT + 1))
            else
                FILTERED_PODS="$FILTERED_PODS $POD"
                INCLUDED_COUNT=$((INCLUDED_COUNT + 1))
            fi
        done

        echo "Pods to collect: $INCLUDED_COUNT, Excluded: $EXCLUDED_COUNT"

        if [ -z "$FILTERED_PODS" ]; then
            echo "No pods remaining after filtering"
            return
        fi

        # Collect logs from each pod into separate files
        for POD in $FILTERED_PODS; do
            LOG_FILE="${LOG_DIR}/px-backup-logs-${POD}-${TIMESTAMP}.log"

            echo "  Collecting logs from pod: $POD"
            kubectl logs -n "$NAMESPACE" "$POD" --since=${INTERVAL}s > "$LOG_FILE" 2>&1 || {
                echo "  Warning: Failed to collect logs from pod: $POD"
                rm -f "$LOG_FILE"
                continue
            }

            # Compress the log file if it has content
            if [ -f "$LOG_FILE" ] && [ -s "$LOG_FILE" ]; then
                gzip -f "$LOG_FILE"
                FILE_SIZE=$(ls -lh "${LOG_FILE}.gz" | awk '{print $5}')
                echo "  Created: ${LOG_FILE}.gz (${FILE_SIZE})"
            else
                echo "  Warning: Log file is empty for pod: $POD"
                rm -f "$LOG_FILE"
            fi
        done

        echo "Log collection completed"
    }

    collect_reporting_data() {
        # Use same 10-digit timestamp as logs (YYYYMMDDHH)
        TIMESTAMP=$(date +%Y%m%d%H)
        REPORTING_FILE="${LOG_DIR}/px-backup-reporting-${TIMESTAMP}.log"

        echo "$(date): Collecting reporting data with timestamp $TIMESTAMP"

        # Get px-backup pod name - use specific label to get the main px-backup pod
        PX_BACKUP_POD=$(kubectl get pods -n "$NAMESPACE" -l "app=px-backup" -o jsonpath='{.items[0].metadata.name}')

        if [ -z "$PX_BACKUP_POD" ]; then
            echo "Warning: No px-backup pod found, skipping reporting data collection"
            return
        fi

        echo "  Fetching reporting data from pod: $PX_BACKUP_POD"

        # Call Reporting API from within px-backup pod
        kubectl exec -n "$NAMESPACE" "$PX_BACKUP_POD" -- \
            curl -s http://localhost:10001/v1/reporting/data > "$REPORTING_FILE" 2>&1 && {

            # Verify file has content
            if [ -f "$REPORTING_FILE" ] && [ -s "$REPORTING_FILE" ]; then
                # Compress the reporting file
                gzip -f "$REPORTING_FILE"
                FILE_SIZE=$(ls -lh "${REPORTING_FILE}.gz" | awk '{print $5}')
                echo "  Created: ${REPORTING_FILE}.gz (${FILE_SIZE})"
            else
                echo "  Warning: Reporting data file is empty"
                rm -f "$REPORTING_FILE"
            fi
        } || {
            echo "  Warning: Failed to fetch reporting data from Reporting API"
            rm -f "$REPORTING_FILE"
        }
    }

    # Main loop
    while true; do
        collect_logs
        collect_reporting_data

        # Trigger log upload via gRPC (uploads both logs and reporting data)
        echo "Triggering log upload..."
        /tmp/grpcurl/grpcurl -plaintext localhost:9090 com.purestorage.parts.loguploader.LogUploadService/UploadLog && {
            echo "Upload triggered successfully"
        } || {
            echo "Warning: Failed to trigger upload, will retry next cycle"
        }

        echo "Sleeping for ${INTERVAL} seconds..."
        sleep ${INTERVAL}
    done
---
# Envoy Proxy Configuration for Logs Upload
apiVersion: v1
kind: ConfigMap
metadata:
  name: px-backup-telemetry-logs-envoy-config
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: px-backup-telemetry
    app.kubernetes.io/name: logs-collector
{{- include "px-central.labels" . | nindent 4 }}
data:
  envoy-config.yaml: |
    admin:
      address:
        socket_address:
          address: 127.0.0.1
          port_value: 9901
    # Node identification (required for SDS)
    node:
      id: "id_logs"
      cluster: "cluster_logs"
    static_resources:
      listeners:
      - name: listener_logs
        address:
          socket_address:
            address: 127.0.0.1
            port_value: 12002
        filter_chains:
        - filters:
          - name: envoy.filters.network.http_connection_manager
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
              stat_prefix: ingress_http
              access_log:
              - name: envoy.access_loggers.stdout
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog
              http_filters:
              - name: envoy.filters.http.router
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
              route_config:
                name: local_route
                virtual_hosts:
                - name: local_service
                  domains: ["*"]
                  routes:
                  - match:
                      prefix: "/"
                    # Add headers to identify the product and cluster
                    request_headers_to_add:
                    # Product name - using "portworx" (same as PX-Enterprise)
                    - header:
                        key: "product-name"
                        value: "portworx"
                    # Appliance ID - using appliance-id from ConfigMap
                    - header:
                        key: "appliance-id"
{{- $telemetryConfig := lookup "v1" "ConfigMap" .Release.Namespace "px-backup-telemetry-config" }}
{{- if $telemetryConfig }}
                        value: {{ index $telemetryConfig.data "appliance-id" | quote }}
{{- else }}
                        value: "unknown"
{{- end }}
                    # Component serial number - identifies this as log upload
                    - header:
                        key: "component-sn"
                        value: "px-backup-log-upload"
                    # Product version - PX-Backup version
                    - header:
                        key: "product-version"
                        value: {{ .Values.pxbackup.version | quote }}
                    route:
                      host_rewrite_literal: {{ .Values.pxbackup.telemetry.restEndpoint }}
                      cluster: cluster_cloud_support
      clusters:
      - name: cluster_cloud_support
        type: STRICT_DNS
        dns_lookup_family: V4_ONLY
        lb_policy: ROUND_ROBIN
        load_assignment:
          cluster_name: cluster_cloud_support
          endpoints:
          - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: {{ .Values.pxbackup.telemetry.restEndpoint }}
                    port_value: 443
        transport_socket:
          name: envoy.transport_sockets.tls
          typed_config:
            "@type": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext
            sni: {{ .Values.pxbackup.telemetry.restEndpoint }}
            common_tls_context:
              # Client certificate configuration using SDS
              tls_certificate_sds_secret_configs:
                name: tls_sds
                sds_config:
                  # Path to SDS configuration file
                  path: /etc/envoy/tls_certificate_sds_secret.yaml
              # Server certificate validation
              validation_context:
                trusted_ca:
                  filename: /etc/ssl/certs/ca-certificates.crt
                match_typed_subject_alt_names:
                - san_type: DNS
                  matcher:
                    exact: {{ .Values.pxbackup.telemetry.restEndpoint }}
---
# SDS Configuration for Dynamic Certificate Loading
apiVersion: v1
kind: ConfigMap
metadata:
  name: px-backup-telemetry-logs-envoy-sds-config
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: px-backup-telemetry
    app.kubernetes.io/name: logs-collector
{{- include "px-central.labels" . | nindent 4 }}
data:
  tls_certificate_sds_secret.yaml: |
    resources:
    - "@type": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret
      name: tls_sds
      tls_certificate:
        certificate_chain:
          filename: /appliance-cert/cert
        private_key:
          filename: /appliance-cert/private_key
---
# Log Upload Service Configuration (ccm.properties)
apiVersion: v1
kind: ConfigMap
metadata:
  name: px-backup-telemetry-logs-upload-config
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: px-backup-telemetry
    app.kubernetes.io/name: logs-collector
{{- include "px-central.labels" . | nindent 4 }}
data:
  ccm.properties: |
    {
      "product_name": "portworx",
      "port": "9090",
      "envoy_port": "12002",
      "logupload": {
        "logfile_patterns": [
          "/var/cores/px-backup-logs-*.log.gz",
          "/var/cores/px-backup-reporting-*.log.gz"
        ],
        "phonehome_hour_range": 744,
        "always_scan_range_days": 7,
        "max_retry_per_hour": 5
      }
    }
{{- end }}


{{- if (and (.Values.monitoring) (eq .Values.monitoring true)) -}}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus-operator
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-operator
subjects:
  - kind: ServiceAccount
    name: prometheus-operator
    namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-operator
  namespace: kube-system
rules:
  - apiGroups:
      - extensions
    resources:
      - thirdpartyresources
    verbs: ["*"]
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs: ["*"]
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - alertmanagers
      - prometheuses
      - prometheuses/finalizers
      - servicemonitors
      - prometheusrules
      - podmonitors
      - thanosrulers
    verbs: ["*"]
  - apiGroups:
      - apps
    resources:
      - statefulsets
    verbs: ["*"]
  - apiGroups: [""]
    resources:
      - configmaps
      - secrets
    verbs: ["*"]
  - apiGroups: [""]
    resources:
      - pods
    verbs: ["list", "delete"]
  - apiGroups: [""]
    resources:
      - services
      - endpoints
    verbs: ["get", "create", "update"]
  - apiGroups: [""]
    resources:
      - nodes
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - namespaces
    verbs: ["list", "watch"]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-operator
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    k8s-app: prometheus-operator
  name: prometheus-operator
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: prometheus-operator
  replicas: 1
  template:
    metadata:
      labels:
        k8s-app: prometheus-operator
    spec:
      containers:
        - args:
            - --kubelet-service=kube-system/kubelet
            - --config-reloader-image=quay.io/coreos/configmap-reload:v0.0.1
          image: quay.io/coreos/prometheus-operator:v0.36.0
          name: prometheus-operator
          ports:
            - containerPort: 8080
              name: http
          resources:
            limits:
              cpu: 200m
              memory: 100Mi
            requests:
              cpu: 100m
              memory: 50Mi
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: prometheus-operator
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  namespace: kube-system
  name: portworx-prometheus-sm
  labels:
    name: portworx-prometheus-sm
spec:
  selector:
    matchLabels:
      name: portworx
  namespaceSelector:
    any: true
  endpoints:
    - port: px-api
      targetPort: 9001
    - port: px-kvdb
      targetPort: 9019
---
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: portworx #This name is important since the Alertmanager pods wont start unless a secret named alertmanager-${ALERTMANAGER_NAME} is created. in this case if would expect alertmanager-portworx secret in the kube-system namespace
  namespace: kube-system
  labels:
    alertmanager: portworx
spec:
  replicas: 3  
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager-portworx
  namespace: kube-system
spec:
  type: ClusterIP
  ports:
    - name: web
      port: 9093
      protocol: TCP
      targetPort: 9093
  selector:
    alertmanager: portworx
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: portworx
    role: prometheus-portworx-rulefiles
  name: prometheus-portworx-rules-portworx.rules.yaml
  namespace: kube-system
spec:
  groups:
  - name: portworx.rules
    rules:
    - alert: PortworxVolumeUsageCritical
      annotations:
        description: {{`Portworx volume {{$labels.volumeid}} on {{$labels.host}} is over 80% used for
          more than 10 minutes.`}}
        summary: {{`Portworx volume capacity is at {{$value}}% used.`}}
      expr: 100 * (px_volume_usage_bytes / px_volume_capacity_bytes) > 80
      for: 5m
      labels:
        issue: {{`Portworx volume {{$labels.volumeid}} usage on {{$labels.host}} is high.`}}
        severity: critical
    - alert: PortworxVolumeUsage
      annotations:
        description: {{`Portworx volume {{$labels.volumeid}} on {{$labels.host}} is over 70% used for
          more than 10 minutes.`}}
        summary: {{`Portworx volume {{$labels.volumeid}} on {{$labels.host}} is at {{$value}}% used.`}}
      expr: 100 * (px_volume_usage_bytes / px_volume_capacity_bytes) > 70
      for: 5m
      labels:
        issue: {{`Portworx volume {{$labels.volumeid}} usage on {{$labels.host}} is critical.`}}
        severity: warning
    - alert: PortworxVolumeWillFill
      annotations:
        description: {{`Disk volume {{$labels.volumeid}} on {{$labels.host}} is over 70% full and has
          been predicted to fill within 2 weeks for more than 10 minutes.`}}
        summary: {{`Portworx volume {{$labels.volumeid}} on {{$labels.host}} is over 70% full and is
          predicted to fill within 2 weeks.`}}
      expr: (px_volume_usage_bytes / px_volume_capacity_bytes) > 0.7 and predict_linear(px_cluster_disk_available_bytes[1h],
        14 * 86400) < 0
      for: 10m
      labels:
        issue: {{`Disk volume {{$labels.volumeid}} on {{$labels.host}} is predicted to fill within
          2 weeks.`}}
        severity: warning
    - alert: PortworxStorageUsageCritical
      annotations:
        description: {{`Portworx storage {{$labels.volumeid}} on {{$labels.host}} is over 80% used
          for more than 10 minutes.`}}
        summary: {{`Portworx storage capacity is at {{$value}}% used.`}}
      expr: 100 * (1 - px_cluster_disk_utilized_bytes / px_cluster_disk_total_bytes)
        < 20
      for: 5m
      labels:
        issue: {{`Portworx storage {{$labels.volumeid}} usage on {{$labels.host}} is high.`}}
        severity: critical
    - alert: PortworxStorageUsage
      annotations:
        description: {{`Portworx storage {{$labels.volumeid}} on {{$labels.host}} is over 70% used
          for more than 10 minutes.`}}
        summary: {{`Portworx storage {{$labels.volumeid}} on {{$labels.host}} is at {{$value}}% used.`}}
      expr: 100 * (1 - (px_cluster_disk_utilized_bytes / px_cluster_disk_total_bytes))
        < 30
      for: 5m
      labels:
        issue: {{`Portworx storage {{$labels.volumeid}} usage on {{$labels.host}} is critical.`}}
        severity: warning
    - alert: PortworxStorageWillFill
      annotations:
        description: {{`Portworx storage {{$labels.volumeid}} on {{$labels.host}} is over 70% full
          and has been predicted to fill within 2 weeks for more than 10 minutes.`}}
        summary: {{`Portworx storage {{$labels.volumeid}} on {{$labels.host}} is over 70% full and
          is predicted to fill within 2 weeks.`}}
      expr: (100 * (1 - (px_cluster_disk_utilized_bytes / px_cluster_disk_total_bytes)))
        < 30 and predict_linear(px_cluster_disk_available_bytes[1h], 14 * 86400) <
        0
      for: 10m
      labels:
        issue: {{`Portworx storage {{$labels.volumeid}} on {{$labels.host}} is predicted to fill within
          2 weeks.`}}
        severity: warning
    - alert: PortworxStorageNodeDown
      annotations:
        description: Portworx Storage Node has been offline for more than 5 minutes
        summary: Portworx Storage Node is Offline.
      expr: max(px_cluster_status_nodes_storage_down) > 0
      for: 5m
      labels:
        issue: Portworx Storage Node is Offline.
        severity: critical
    - alert: PortworxQuorumUnhealthy
      annotations:
        description: Portworx cluster Quorum Unhealthy for more than 5 minutes.
        summary: Portworx Quorum Unhealthy.
      expr: max(px_cluster_status_cluster_quorum) > 1
      for: 5m
      labels:
        issue: Portworx Quorum Unhealthy.
        severity: critical
    - alert: PortworxMemberDown
      annotations:
        description: Portworx cluster member(s) has(have) been down for more than
          5 minutes.
        summary: Portworx cluster member(s) is(are) down.
      expr: (max(px_cluster_status_cluster_size) - count(px_cluster_status_cluster_size))
        > 0
      for: 5m
      labels:
        issue: Portworx cluster member(s) is(are) down.
        severity: critical
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources:
      - nodes
      - services
      - endpoints
      - pods
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources:
      - configmaps
    verbs: ["get"]
  - nonResourceURLs: ["/metrics", "/federate"]
    verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: kube-system
---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
  namespace: kube-system
spec:
  replicas: 2
  logLevel: debug
  serviceAccountName: prometheus
  alerting:
    alertmanagers:
      - namespace: kube-system
        name: alertmanager-portworx
        port: web
  serviceMonitorSelector:
    matchLabels:
      name: portworx-prometheus-sm
  ruleSelector:
    matchLabels:
      role: prometheus-portworx-rulefiles
      prometheus: portworx
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: kube-system
spec:
  type: ClusterIP
  ports:
    - name: web
      port: 9090
      protocol: TCP
      targetPort: 9090
  selector:
    prometheus: prometheus
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: grafana-dashboards
  namespace: kube-system
  labels:
    role: grafana-dashboardfiles
    grafana: portworx
data:
    {{- (.Files.Glob "files/portworx-cluster-dashboard.json").AsConfig | nindent 2 }}
    {{- (.Files.Glob "files/portworx-node-dashboard.json").AsConfig | nindent 2 }}
    {{- (.Files.Glob "files/portworx-volume-dashboard.json").AsConfig | nindent 2 }}
    {{- (.Files.Glob "files/portworx-etcd-dashboard.json").AsConfig | nindent 2 }}
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: grafana-source-config
  namespace: kube-system
  labels:
    role: grafana-source-configfiles
    grafana: portworx
data:
  datasource.yml: |-
    # config file version
    apiVersion: 1
    # list of datasources that should be deleted from the database
    deleteDatasources:
      - name: prometheus
        orgId: 1
    # list of datasources to insert/update depending
    # whats available in the database
    datasources:
      # <string, required> name of the datasource. Required
    - name: prometheus
      # <string, required> datasource type. Required
      type: prometheus
      # <string, required> access mode. direct or proxy. Required
      access: proxy
      # <int> org id. will default to orgId 1 if not specified
      orgId: 1
      # <string> url
      url: http://prometheus:9090
      # <string> database password, if used
      password:
      # <string> database user, if used
      user:
      # <string> database name, if used
      database:
      # <bool> enable/disable basic auth
      basicAuth: true
      # <string> basic auth username
      basicAuthUser: admin
      # <string> basic auth password
      basicAuthPassword: foobar
      # <bool> enable/disable with credentials headers
      withCredentials:
      # <bool> mark as default datasource. Max one per org
      isDefault:
      # <map> fields that will be converted to json and stored in json_data
      jsonData:
        graphiteVersion: "1.1"
        tlsAuth: false
        tlsAuthWithCACert: false
      # <string> json object of data that will be encrypted.
      secureJsonData:
        tlsCACert: "..."
        tlsClientCert: "..."
        tlsClientKey: "..."
      version: 1
      # <bool> allow users to edit datasources from the UI.
      editable: true
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: grafana-dashboard-config
  namespace: kube-system
  labels:
    role: grafana-dashboard-configfiles
    grafana: portworx
data:
  dashboards.yml: |-
    apiVersion: 1
    providers:
    - name: 'default'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: false
      editable: true
      options:
        path: /var/lib/grafana/dashboards
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: kube-system
  labels:
    app: grafana
spec:
  type: ClusterIP
  ports:
    - port: 3000
  selector:
    app: grafana
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: kube-system
  labels:
    app: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
        - image: grafana/grafana:7.1.3
          name: grafana
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: 100m
              memory: 100Mi
            requests:
              cpu: 100m
              memory: 100Mi
          readinessProbe:
            httpGet:
              path: /login
              port: 3000
          volumeMounts:
            - name: grafana-dash-config
              mountPath: /etc/grafana/provisioning/dashboards
            - name: dashboard-templates
              mountPath: /var/lib/grafana/dashboards
            - name: grafana-source-config
              mountPath: /etc/grafana/provisioning/datasources
      volumes:
        - name: grafana-source-config
          configMap:
            name: grafana-source-config
        - name: grafana-dash-config
          configMap:
            name: grafana-dashboard-config
        - name: dashboard-templates
          configMap:
            name: grafana-dashboards
{{- end }}
